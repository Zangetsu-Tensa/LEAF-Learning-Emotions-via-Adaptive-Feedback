# ğŸ® PPO-Breakout â€” Proximal Policy Optimization from Scratch for Atari Environments

> _"Learning to play like a human â€” one clipped gradient at a time."_  

This repository implements **Proximal Policy Optimization (PPO)** entirely **from scratch** in **PyTorch**, trained on **Atari Breakout (NoFrameskip-v4)** with frame-stacking, grayscale preprocessing, and parallel actor environments.  

Itâ€™s built for clarity and depth â€” every part of PPO is explicitly implemented and optimized for understanding, not just performance.

---

## ğŸ§  Overview

**Proximal Policy Optimization (PPO)** is one of the most stable and widely used reinforcement learning algorithms introduced by **OpenAI** (Schulman et al., 2017).  
This implementation captures the core ideas behind PPO â€” including **clipped policy objectives**, **advantage estimation (GAE)**, and **actor-critic training**, all without relying on external RL libraries.

---

## ğŸš€ Key Features

- ğŸ§© **From-Scratch PPO Implementation** â€” no Stable Baselines or RLlib dependencies  
- âš™ï¸ **Actorâ€“Critic CNN Architecture** for visual Atari environments  
- ğŸŒ€ **Parallelized Multi-Actor Setup** for efficient experience collection  
- ğŸšï¸ **Generalized Advantage Estimation (GAE)** for stable gradient updates  
- ğŸ”’ **Clipped Surrogate Objective** for reliable policy improvement  
- ğŸ“‰ **Value Function Clipping** (from PPO2 improvements, 2020)  
- ğŸ“ˆ **Auto LR Scheduling & Reward Plotting**  

---

## ğŸ—ï¸ Architecture Overview

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ENVIRONMENT â”‚
â”‚ (BreakoutNoFrameskip-v4, FrameStacked 4x) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â–¼
Convolutional Encoder
â”‚
â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Shared Feature h â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ â”‚
â–¼ â–¼
Actor (Ï€) Critic (V)
â”‚ â”‚
â–¼ â–¼
Action State Value